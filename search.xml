<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>About Me</title>
      <link href="/2023/09/20/about/"/>
      <url>/2023/09/20/about/</url>
      
        <content type="html"><![CDATA[<h1>About Me</h1><p>A PhD student in the school of Artificial Intelligence and Automation, Huazhong University of Science and Technology. My research interests include SLAM, visual-inertial odometry, deep learning, and robotics.</p><h2 id="Education">Education</h2><ul><li>2016.09 - 2020.06, Bachelor, Huazhong University of Science and Technology, China</li><li>2020.09 - 2022.06, Master, Huazhong University of Science and Technology, China</li><li>2022.09 - present, PhD student, Huazhong University of Science and Technology, China</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Publications</title>
      <link href="/2023/09/20/camera-control/"/>
      <url>/2023/09/20/camera-control/</url>
      
        <content type="html"><![CDATA[<h1>Publications</h1><p>[1] <strong>Y. Lin</strong>, S. Wang, Y. Jiang, B. Han, “Breaking of brightness consistency in optical flow with a lightweight CNN network”, Submission in progress (ICRA2024)<br><a href="https://www.bilibili.com/video/BV1gz4y1V77M/?vd_source=4dd69fa6d40221a0fa0733def5c4708a#reply576226249">Video</a> <a href="https://github.com/linyicheng1/LET-NET">Code</a></p><p>[2] <strong>Y. Lin</strong>, Z. Li, T. Zhang, X. Chen, B. Han, “FKM-EVO: An Image Matching Benchmark and Evaluation based on SLAM Matching Modes”, Submission in progress (TII)<br><a href="/ImageMatchingBenchmark.html">Web</a> <a href="https://www.bilibili.com/video/BV1GP4y1q7qm/?spm_id_from=333.999.0.0">Video</a> <a href="https://github.com/linyicheng1/A-ORB-SLAM2">Code</a></p><p>[3] B. Han, <strong>Y. Lin</strong>, Y. Dong, H. Wang, T. Zhang and C. Liang, “Camera Attributes Control for Visual Odometry With Motion Blur Awareness,” in IEEE/ASME Transactions on Mechatronics, vol. 28, no. 4, pp. 2225-2235, Aug. 2023<br><a href="https://ieeexplore.ieee.org/document/10040760">Paper</a>  <a href="https://www.bilibili.com/video/BV1GP4y1q7qm/?spm_id_from=333.999.0.0">Video</a></p><p>[4] <strong>Y. Lin</strong>, Z. Li, Y. Jiang and B. Han, “MLPoint:An Efficient Multi-Layer Feature Point with Resource Awareness,” Submission in progress (CVPR2024)</p><h1>Awards and Honors</h1><p>[1] ICCV 2023 SLAM Challage 2nd in VIO<br>[2] RoboMaster 2020 Second National Award (Team leader)<br>[3] RoboMaster 2019 Second National Award (Drone Group leader)<br>[4] RoboCup 2018 Second National Award</p><h1>Projects</h1><h4 id="laser-localization">laser_localization</h4><div>    <div style="float:left;width:20%">        <img src="/img/proj_laser.png" width="80%">    </div>    <div style="float:left;width:80%">        laser_localization is a 3D LiDAR localization algorithm applied to small area scenes, typical application scenarios are industrial parks, neighborhoods or substations, etc. It combines 3D laser point cloud, wheeled odometer and IMU angle information to achieve high precision real-time positioning.     </div></div><p><a href="https://github.com/linyicheng1/laser_localization">Code</a> <a href="https://www.bilibili.com/video/BV12P4y1m7nH/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=4dd69fa6d40221a0fa0733def5c4708a">Video1</a> <a href="https://www.bilibili.com/video/BV1z3411M7TJ/?spm_id_from=333.999.0.0&amp;vd_source=4dd69fa6d40221a0fa0733def5c4708a">Video2</a></p><br><h4 id="Structured-Road-Cruise-Path-Planning">Structured Road Cruise Path Planning</h4><div>    <div style="float:left;width:20%">        <img src="/img/proj_struct.png" width="80%">    </div>    <div style="float:left;width:80%">        Global paths are searched in structured roads using A*, then Bessel curves are used to smooth the paths and assign speeds, and the mpc model predictive control tracks the smoothed trajectories.    </div></div><p><a href="https://www.bilibili.com/video/BV1Zh4y1g7Ap/?spm_id_from=333.999.0.0&amp;vd_source=4dd69fa6d40221a0fa0733def5c4708a">Video</a></p><br><h1>Application</h1><h4 id="Railroad-signal-room-inspection-robot">Railroad signal room inspection robot</h4><div>    <div style="float:left;width:20%">        <img src="/img/app_signal.png" width="80%">    </div>    <div style="float:left;width:80%">        An inspection robot applied to a railroad control signal room, using 2d radar for positioning and navigation. A modified cartographer is used for map construction and localization. In addition, hybrid A* algorithm is used for global path planning and teb algorithm is used for local path planning. The captured images are processed in real time by YOLOv5 and the values of devices.    </div></div><p><a href="https://www.bilibili.com/video/BV1Tv411G7mT/?spm_id_from=333.999.0.0&amp;vd_source=4dd69fa6d40221a0fa0733def5c4708a">Video</a></p><br><h4 id="Outdoor-Electricity-Inspection-Robot">Outdoor Electricity Inspection Robot</h4><div>    <div style="float:left;width:20%">        <img src="/img/app_jihan.png" width="80%">    </div>    <div style="float:left;width:80%">        An inspection robot applied to the state of outdoor power substations. A 3d radar is used for positioning and navigation. A map is constructed using hdl_graph_slam and then laser_localization project is used for localization. The global paths are constructed using the A* algorithm in a graph network based on straight lines and inspection points.     </div></div><p><a href="">Video</a></p><br><h4 id="Chemical-factory-inspection-robot">Chemical factory inspection robot</h4><div>    <div style="float:left;width:20%">        <img src="/img/app_hua.png" width="80%">    </div>    <div style="float:left;width:80%">        Inspection robots for methanol chemical plants with the highest level of explosion protection Exd II C.    </div></div>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2023年6-8月SLAM论文速递</title>
      <link href="/2023/08/29/23-8-SLAM/"/>
      <url>/2023/08/29/23-8-SLAM/</url>
      
        <content type="html"><![CDATA[<h2 id="大规模VR定位方案">大规模VR定位方案</h2><p>A Low-cost and Scalable Framework to Build Large-Scale Localization Benchmark for Augmented Reality</p><h2 id="深度预测">深度预测</h2><p>LightDepth: Single-View Depth Self-Supervision from Illumination Decline</p><h2 id="视觉定位">视觉定位</h2><p>Wide-Area Geolocalization with a Limited Field of View Camera in Challenging Urban Environments</p><p>AnyLoc: Towards Universal Visual Place Recognition</p><p>Visual-inertial Loose Coupling for Resilient and Lightweight Direct Visual Localization</p><h2 id="Nerf">Nerf</h2><p>VERF: Runtime Monitoring of Pose Estimation with Neural Radiance Fields</p><h2 id="非刚性SLAM">非刚性SLAM</h2><p>SimCol3D–3D Reconstruction during Colonoscopy Challenge</p><h2 id="特征匹配">特征匹配</h2><p>AffineGlue: Joint Matching and Robust Estimation</p><p>FeatureBooster: Boosting Feature Descriptors with a Lightweight Neural Network Supplemental Material</p><p>PATS: Patch Area Transportation with Subdivision for Local Feature Matching</p><p>Two-View Geometry Scoring Without Correspondences</p><h2 id="VIO">VIO</h2><p>Closed-Form Solution for Absolute Scale Velocity Determination Combining Inertial Measurements and a Single Feature</p><p>Optimization-based VINS: Consistency, Marginalization, and FEJ</p><p>HDVIO: Improving Localization and Disturbance Estimation with Hybrid Dynamics VIO</p><p>Online Self-Calibration for Visual-Inertial Navigation: Models, Analysis, and Degeneracy</p><p>Fast Monocular Visual-Inertial Initialization Leveraging Learned Single-View Depth</p><h2 id="后端优化">后端优化</h2><p>Scale jump-aware pose graph relaxation for monocular SLAM with re-initializations</p><h2 id="数据集">数据集</h2><p>SubT-MRS: A Subterranean, Multi-Robot, Multi-Spectral and Multi-Degraded Dataset for Robust SLAM</p><p>M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset</p><h2 id="线面特征">线面特征</h2><p>Multi-Session, Localization-oriented and Lightweight LiDAR Mapping Using Semantic Lines and Planes</p><p>Efficient Visual-Inertial Navigation with Point-Plane Map</p><p>Hong Kong World: Leveraging Structural Regularity for Line-based SLAM</p><h2 id="事件相机">事件相机</h2><p>E-Calib: A Fast, Robust and Accurate Calibration Toolbox for Event Cameras</p><h2 id="地图构建">地图构建</h2><p>H2-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid Representation</p><h2 id="回环检测">回环检测</h2><p>Towards View-invariant and Accurate Loop Detection Based on Scene Graph</p><h2 id="语义SLAM">语义SLAM</h2><p>TextSLAM: Visual SLAM with Semantic Planar Text Features</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Research_Direction_in_SLAM</title>
      <link href="/2023/08/26/Research_Direction_in_SLAM/"/>
      <url>/2023/08/26/Research_Direction_in_SLAM/</url>
      
        <content type="html"><![CDATA[<h1>视觉SLAM中的研究点总结 (更新中)</h1><p>本文为个人学习、阅读文献以及工程项目实践中对于SLAM算法目前存在的问题和一些可行的论文创新点的总结。对于其中的内容有不当之处欢迎在Issues中进行讨论。</p><blockquote><p><strong>目录</strong></p><ol><li>概述</li><li>针对视觉SLAM系统各模块的研究<ol><li>信息采集——新式传感器/多传感器</li><li>信息提取——特征提取</li><li>信息关联——特征匹配</li><li>信息处理（直接法）—— 图像灰度对齐估计位姿</li><li>信息处理（特征点）—— 特征点关联估计位姿</li><li>信息处理（后端优化）—— 基于关键帧结构的后端优化</li><li>信息处理（地图）—— 地图构建</li><li>端到端的多任务处理</li></ol></li><li>视觉SLAM相关的其他任务<ol><li>先构建视觉特征地图后定位的视觉定位算法</li></ol></li></ol></blockquote><h2 id="News">News</h2><ul><li>2023.7.28 参加第四届SLAM论坛，在交流学习后，对本文整体进行了进一步的完善。</li></ul><ol><li>进一步理解直接法的优势与面临的挑战</li><li>Nerf 与 SLAM的关系</li><li>未来的 SLAM 形态</li></ol><ul><li>2023.5.15 更新了<a href="https://github.com/linyicheng1/Research_Direction_in_SLAM/blob/main/theme/%E7%9B%B4%E7%BA%BF%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96.md">手工直线特征提取</a>的综述</li></ul><h2 id="概述">概述</h2><p>视觉SLAM算法目前遇到的困境在于理论的成熟度与实际的落地应用情况的不匹配。在经典的ORB-SLAM2,ORB-SLAM3等解决方案面前似乎SLAM是一个已经被解决的问题。但是在众多的移动机器人以及各种应用如VR，AR中却迟迟没有落地，这是由于其在鲁棒性和精度等方面依旧纯在明显的短板。另一方面，近年来越来越多的研究者加入这一领域，却发现前辈们的工作理论复杂度高、工程实现难、算法改进更是无从谈起，因此跟随书籍、课程学习一段相关基础知识便陷入无从下手的困境。因此，本文抛砖引玉，提出一些SLAM中可以去深入挖掘的一些点，给研究者提供一个思路。本文会随着个人的学习、实践中的成长，逐步修改添加其中的一些问题。也欢迎各位加入共同完善这一工作。</p><p>对下文中提出的问题，会给出一个简单的描述，然后给出一个简单的创新点，最后凭借个人主观论断给出一个难度评估。部分问题会在一个单独的文档中给出详细的相关文献甚至领域发展脉络为进一步的研究提供依据，并将链接附在表格最后一列。</p><h2 id="信息采集——新式传感器-多传感器">信息采集——新式传感器/多传感器</h2><p>目标：<strong>在各种极端环境下采集到尽量多的数据</strong></p><p>关键字：<strong>互补</strong></p><p>信息采集上需要考虑目前常用的视觉传感器的不足之处，采用可以补充这一部分信息的传感器并融合或者单独使用在特定环境下对现有算法进行补充。</p><p>如：</p><ol><li>视觉 + IMU<ol><li>单目视觉具有尺度不确定性，添加IMU中的加速度计补充尺度信息</li><li>低级视觉特征对视角不变性差，因此添加IMU中的陀螺仪信息,提高特征关联的效率</li></ol></li><li>RGB相机 + 红外相机<ol><li>RGB 相机对光照变化敏感，在黑夜环境中无法使用，因此添加红外相机进行互补</li></ol></li><li>鱼眼相机/360°相机<ol><li>传统相机视角小，快速运动下若有几帧数据没有及时处理视角转到另一个角度后必然导致跟踪失败。而360°相机或者尽量大广角的相机则可以更好的避免这一点。</li></ol></li></ol><h3 id="单一传感器">单一传感器</h3><h4 id="鱼眼镜头">鱼眼镜头</h4><p>鱼眼镜头的优势在于单一传感器即可获取超大视野，一般可以超过180°。但是缺点也很明显，即畸变严重、标定复杂。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>鱼眼镜头标定</td><td>提出新相机模型替代针孔模型</td><td>考虑鱼眼镜头结构提出更快、表达能力强的相机模型</td><td>:star::star::star::star:</td><td></td></tr><tr><td>鱼眼镜头标定</td><td>现有模型下提出标定方案</td><td>鱼眼相机畸变严重，传统标定特征失效</td><td>:star::star::star:</td><td></td></tr><tr><td>鱼眼畸变下的特征提取</td><td>畸变导致图像变化，如直线特征等特征提取困难</td><td>基于鱼眼图像的特征提取算法</td><td>:star::star::star:</td><td></td></tr></tbody></table><h4 id="卷帘相机">卷帘相机</h4><p>SLAM算法中常用的是全局曝光相机，这种相机同时采集得到所有图像内所有点的像素信息，因此没有果冻效应。但是卷帘相机有如下的优势：</p><ol><li>动态范围大，卷帘相机的动态范围远远大于相同价位的全局曝光相机，在高动态光照场景下具有巨大的优势</li><li>应用普遍，在手机、相机等大部分的消费电子设备中采用的均为卷帘相机。</li><li>图像质量更高</li></ol><p>缺点也是很明显的，即<strong>果冻效应</strong></p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>卷帘相机应用</td><td>提出果冻效应补偿方案</td><td>考虑相机果冻效应模型，对SLAM算法中的特征进行位姿补偿</td><td>:star::star::star:</td><td></td></tr><tr><td>卷帘相机应用</td><td>提出果冻效应补偿方案</td><td>考虑相机果冻效应模型，加入到优化项中同时优化</td><td>:star::star::star::star:</td><td></td></tr></tbody></table><p>果冻效应不仅仅表现在图像像素位置的移动，还会导致每一行的图像成像曝光参数不一致，即整张图像不是由统一的参数采集得到，导致灰度一致性、或者光度一致性差。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>卷帘相机与光度不变性冲突</td><td>基于卷帘相机中的不变量改进光流/直接法</td><td>卷帘相机中的直接法</td><td>:star::star::star::star:</td><td></td></tr></tbody></table><h4 id="360相机">360相机</h4><p>基于目前商用的360相机做SLAM算法研究。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>360相机中的特征</td><td>考虑特殊相机畸变导致特征提取性能下降</td><td>考虑特殊的相机模型，对图像特征进行提取</td><td>:star::star::star::star:</td><td></td></tr><tr><td>360相机中的多视图几何</td><td>提出经典问题在360相机中的求解方法</td><td>考虑相机模型，重新推导传统的多视图几何问题</td><td>:star::star::star::star:</td><td></td></tr></tbody></table><h4 id="相机参数的主动控制">相机参数的主动控制</h4><blockquote><p>相机在突然进入高光强环境或者突然进入很暗的场景中，会出现纯白、纯黑的图像。<br>相机在黑暗环境下会产生运动模糊，使得SLAM算法失效</p></blockquote><p>因此传感器本身的参数设置对于图像质量的影响也是至关重要,通常包括<strong>曝光时间</strong>和<strong>相机增益</strong>两个参数。</p><p>曝光时间：黑暗环境下增加进光量，但会导致图像产生运动模糊。</p><p>相机增益：对图像信号进行硬件放大，会同时放大噪声导致图像质量下降。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>相机参数的主动控制</td><td>提出控制方法获取曝光合适、无模糊的图像</td><td>考虑SLAM运动场景，预测控制相机参数获得更优图像</td><td>:star::star::star:</td><td></td></tr></tbody></table><h4 id="事件相机">事件相机</h4><p>事件相机是一个新型传感器，动态感知光照变化。<br>其优点在于：低延迟、无运动模糊、低冗余数据、数据异步、高动态范围、低功耗。<br>其缺点在于：数据稀疏、数据量随场景与运动变化显著、噪声显著。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>运动无关的特征表示</td><td>对于特征跟踪，事件相机的输出与运动方式有关，寻找运动不变性事件特征有利于跟踪</td><td>运动不变性事件表示</td><td>:star::star:</td><td></td></tr><tr><td>稠密建图</td><td>事件相机的三维重建只能建立稀疏边缘</td><td></td><td>:star::star::star::star:</td><td></td></tr><tr><td>回环检测</td><td>事件相机当前大多是VO/VIO，缺少回环模块</td><td>基于时序数据的回环检测</td><td>:star::star::star:</td><td></td></tr><tr><td>语义地图</td><td>事件相机的建图缺少语义信息，无法指导高层次任务</td><td>结合基于事件的物体识别、分割等算法</td><td>:star::star::star:</td><td></td></tr><tr><td>SLAM鲁棒性研究</td><td>事件相机适用于高速、纹理较充足场景，但低速场景下数据较少，稳定性差</td><td>传感器融合、新的SLAM框架</td><td>:star::star::star:</td><td></td></tr></tbody></table><h3 id="多传感器融合">多传感器融合</h3><h4 id="VIO-（视觉惯性里程计）">VIO （视觉惯性里程计）</h4><p>VIO 是最为经典的多传感器组合，目前研究较为全面且完善。</p><h4 id="多相机的全景相机">多相机的全景相机</h4><p>多个相机朝着不同方向排布，共同获取得到全景视野的信息。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>离线外参标定问题</td><td>提出高效高精度的外参标定算法</td><td>外参标定算法流程</td><td>:star::star::star:</td><td></td></tr><tr><td>在线外参标定问题</td><td>将外参添加到BA优化中在线更新</td><td>考虑外参变化的多目SLAM算法</td><td>:star::star::star::star:</td><td></td></tr><tr><td>高效特征提取匹配</td><td>多相机带来前端处理数据加大</td><td>提出特征处理加速方法</td><td>:star::star::star::star:</td><td></td></tr></tbody></table><h4 id="RGB相机-红外相机">RGB相机 + 红外相机</h4><h3 id="其他">其他</h3><h4 id="主动光源">主动光源</h4><p>模拟人类视觉方案，在前置灯光条件下，对SLAM算法在夜间的性能进行改进与提高。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>主动光源模型</td><td>光源模型标定</td><td>提出光场分布模型的标定算法</td><td>:star::star::star::star:</td><td></td></tr><tr><td>主动光源外参</td><td>光源位置与相机外参标定</td><td>提出光场分布与相机位置关系的标定算法</td><td>:star::star::star::star:</td><td></td></tr><tr><td>主动光源SLAM</td><td>考虑光场模型的SLAM算法</td><td>将光场分布先验应用到特征匹配中</td><td>:star::star::star::star:</td><td></td></tr><tr><td>主动光源SLAM</td><td>兼顾高效且低照度性能的特征匹配算法</td><td>新环境下也能很好工作的匹配算法</td><td>:star::star::star::star:</td><td></td></tr></tbody></table><h2 id="信息提取——特征提取">信息提取——特征提取</h2><p>目标：<strong>在既定计算资源提取包含尽量多且精确的信息</strong></p><p>关键字：<strong>效率</strong></p><p>特征匹配中的直接法可以认为直接把原始像素作为特征，这是最为底层的信息。而点特征、直线特征和语义特征等则是包含的语义信息量逐步增加。<br>其中低级特征的对比如表</p><table><thead><tr><th>方法</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>低级特征</td><td>1. 精度高 2. 精度高</td><td>1. 对视角、光照等变化不鲁棒 2.关联难度大，非常依赖稳定的相机运动</td></tr><tr><td>高级特征</td><td>1. 视角、光照等变化更鲁棒 2. 在无先验运动信息情况下匹配稳定，能提高系统整体稳定性</td><td>1. 精度略低 2. 消耗大量计算资源</td></tr></tbody></table><h3 id="点特征">点特征</h3><p>点特征与深度学习结合已经被广泛应用在SfM等任务中，但是在SLAM中的应用还未成为主流。这是由于基于学习的特征点在如下几个方面存在不足：</p><ol><li>计算量大，特征点提取的计算量大大增加，难以在移动端的CPU上实时运行。</li><li>精度不高，特征点提取的精度不如传统的特征点提取算法。</li></ol><p>由于SLAM系统运行在图像序列中，因此前后帧的信息、地图构建得到的深度信息等都可以作为先验信息。极大的降低了匹配的难度，因此现有方法基本属于性能过剩，实时性不足的状态。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>高效学习特征提取</td><td>CPU下的实时特征</td><td>针对图像序列进行训练，考虑实时性，降低网络复杂度</td><td>:star::star::star:</td><td></td></tr><tr><td>精确学习特征提取</td><td>提高特征位置精度</td><td>用局部少量的信息确定特征点位置，并用周围较多的信息进行描述</td><td>:star::star::star:</td><td></td></tr><tr><td>大雾/水下环境的特征提取</td><td>提高特征对环境的鲁棒性</td><td>大雾环境下的特征点得分快速降低，现有方法不能自适应提取</td><td>:star::star::star:</td><td></td></tr></tbody></table><h3 id="直线特征">直线特征</h3><p>直线特征提取的传统方法前几年被广泛的添加到各种点特征的SLAM中，并在无纹理、结构化环境中取得了更优的效果。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>传统方法的改进</td><td>1. 卷精度 2. 卷效率 3. 卷挑战性环境（鲁棒性）</td><td>\</td><td>:star::star::star::star:</td><td><a href="https://github.com/linyicheng1/Research_Direction_in_SLAM/blob/main/theme/%E7%9B%B4%E7%BA%BF%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96.md">手工直线提取综述</a></td></tr><tr><td>学习方法改进</td><td></td><td></td><td></td><td></td></tr></tbody></table><h3 id="语义特征">语义特征</h3><p>语义特征是指在图像中具有语义信息的特征，如车辆、行人、建筑等。这些特征在SLAM中的应用可以帮助系统更好的理解环境，从而提高系统的鲁棒性，是目前的热点之一，也是未来SLAM发展的方向。</p><h2 id="信息关联——特征匹配">信息关联——特征匹配</h2><p>目标：<strong>在不同的匹配阶段，得到足够的特征关联信息</strong></p><p>关键字：<strong>多阶段需求</strong></p><p>特征匹配在SLAM中分为多次进行，并且有的只针对部分关键帧执行。分为如下几个阶段：</p><ol><li>帧间匹配，连续两帧图像之间的匹配</li><li>关键帧和当前帧匹配</li><li>关键帧和关键帧之间的匹配</li><li>关键帧和回环帧之间的匹配</li></ol><table><thead><tr><th>匹配类型</th><th>执行频率</th><th>实时性需求</th><th>性能需求</th></tr></thead><tbody><tr><td>帧间匹配</td><td>高</td><td>高</td><td>低</td></tr><tr><td>关键帧和当前帧</td><td>一般</td><td>一般</td><td>较高</td></tr><tr><td>关键帧和关键帧</td><td>较高</td><td>一般</td><td>一般</td></tr><tr><td>关键帧和回环帧</td><td>低</td><td>较低</td><td>较高</td></tr></tbody></table><h3 id="稀疏光流法">稀疏光流法</h3><p>光流法很好的利用了相邻帧之间的信息，能够在小运动序列中以很低的成本获得很好的效果。但是随着图像序列难度的加大，光流法的性能会急剧下降。因此如何提高光流法对光照、大视角变化的鲁棒性是一个值得研究的问题。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>光流法改进</td><td>提高光流法的鲁棒性</td><td>修正灰度不变性假设，提出新的光流法，可以从学习和传统的方法考虑</td><td>:star::star::star::star:</td><td></td></tr></tbody></table><h3 id="性能递增的特征匹配">性能递增的特征匹配</h3><p>对于前端连续两帧之间的匹配，关键帧和当前帧，关键帧之间以及回环帧与关键帧之间的匹配需求是不一样的。因此如何根据不同的匹配需求采用计算量尽可能低的匹配方法是一个值得研究的问题。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>特征匹配</td><td>提高匹配效率</td><td>在SLAM系统的不同阶段，采用不同的匹配方法，保证整体的计算量最小</td><td>:star::star::star::star::star:</td><td></td></tr></tbody></table><h2 id="信息处理（特征点）——-特征点关联估计位姿">信息处理（特征点）—— 特征点关联估计位姿</h2><p>基于特征点的关联求解局部BA问题，或者仅优化位姿问题非常的成熟和有效，但也涌现了一些基于神经网络的求解器，能够更快速的求解复杂问题。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>新求解器应用</td><td>基于神经网络的方程求解器改进局部BA</td><td>考虑CVPR2022最佳论文之类的神经网络求解方法，将传统SLAM中初始化或者局部BA方法替换掉，提高计算速度</td><td>:star::star::star::star::star:</td><td></td></tr></tbody></table><h2 id="信息处理（直接法）——-图像灰度对齐估计位姿">信息处理（直接法）—— 图像灰度对齐估计位姿</h2><p>直接法是一种优势和劣势都十分明显的有一类方法，从性能上来说，它能够提供足够高质量的实时半稠密地图，但是强烈依赖于光度不变性假设，从易用程度的角度来说，直接法相关代码更复杂，理论更深入，因此对于直接法改进的相关工作远远小于基于特征点法。</p><h3 id="光照鲁棒性的研究">光照鲁棒性的研究</h3><p>由于直接法对光度一致性有较高的要求，这并不总是满足，因此对其光照性能的改进是重要的一环。</p><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>光照鲁棒的直接法</td><td>削弱过强的光度一致性的假设</td><td></td><td>:star::star::star::star::star::star:</td><td></td></tr></tbody></table><h2 id="信息处理（地图）——-地图构建">信息处理（地图）—— 地图构建</h2><h3 id="基于深度学习的深度估计">基于深度学习的深度估计</h3><p>人类在使用一只眼睛且只观看单张图片的场景下依旧能够分辨绝大部分场景的大致深度信息，而视觉SLAM算法则需要采用两帧之间的视差三角化同时估计位姿和深度。因此提出基于CNN网络的学习方法，得到单帧的大致深度信息，利用这个深度信息辅助视觉SLAM系统进行快速且尺度一致的深度估计。当然目前基于学习的深度估计存在两个主要问题：</p><ol><li>精度较低，直接估计得到的深度信息往往没有基于视差优化的准确，这会导致整体系统轨迹误差变大</li><li>效率较低，毕竟是CNN的方法目前也有叫多相关工作进行提速。</li></ol><table><thead><tr><th>研究问题</th><th>动机</th><th>创新点</th><th>难度</th><th>详细信息</th></tr></thead><tbody><tr><td>深度估计的辅助的SLAM</td><td>提高融合之后的轨迹精度</td><td></td><td>:star::star::star::star::star::star:</td><td></td></tr></tbody></table><h3 id="nerf-在地图构建中的应用">nerf 在地图构建中的应用</h3><h2 id="端到端的多任务处理">端到端的多任务处理</h2><h3 id="端到端的提取与匹配任务">端到端的提取与匹配任务</h3><h3 id="端到端的VO算法">端到端的VO算法</h3>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
